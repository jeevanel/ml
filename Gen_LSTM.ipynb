{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Quadro P5000 (CNMeM is disabled, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5110, 6021))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dropout, LSTM, Activation, Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in file 144424\n",
      "Total unique characters in file  47\n",
      "\n",
      "\n",
      "Char lookup  {'t': 0, '“': 1, 'j': 2, '-': 3, '!': 4, '‘': 5, 'p': 6, 'h': 7, 'd': 8, '0': 9, '*': 10, 'b': 11, '\\n': 12, 'c': 13, '_': 14, 's': 15, '[': 16, 'a': 17, '’': 18, 'i': 19, 'v': 20, '.': 21, 'n': 22, 'x': 23, 'z': 24, 'y': 25, '?': 26, 'g': 27, ']': 28, ' ': 29, 'u': 30, 'f': 31, 'q': 32, 'l': 33, '3': 34, 'r': 35, '”': 36, ':': 37, '(': 38, ')': 39, 'o': 40, ',': 41, 'w': 42, 'e': 43, 'm': 44, 'k': 45, ';': 46}\n",
      "\n",
      "\n",
      "Int lookup  {0: 't', 1: '“', 2: 'j', 3: '-', 4: '!', 5: '‘', 6: 'p', 7: 'h', 8: 'd', 9: '0', 10: '*', 11: 'b', 12: '\\n', 13: 'c', 14: '_', 15: 's', 16: '[', 17: 'a', 18: '’', 19: 'i', 20: 'v', 21: '.', 22: 'n', 23: 'x', 24: 'z', 25: 'y', 26: '?', 27: 'g', 28: ']', 29: ' ', 30: 'u', 31: 'f', 32: 'q', 33: 'l', 34: '3', 35: 'r', 36: '”', 37: ':', 38: '(', 39: ')', 40: 'o', 41: ',', 42: 'w', 43: 'e', 44: 'm', 45: 'k', 46: ';'}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 256)               311296    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 323,375\n",
      "Trainable params: 323,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0649Epoch 00001: loss improved from inf to 3.06498, saving model to weights-01-3.0650.hdf5\n",
      "144324/144324 [==============================] - 113s 780us/step - loss: 3.0650\n",
      "Epoch 2/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0494- ETA: - ETA: 1s - Epoch 00002: loss improved from 3.06498 to 3.04942, saving model to weights-02-3.0494.hdf5\n",
      "144324/144324 [==============================] - 114s 789us/step - loss: 3.0494\n",
      "Epoch 3/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0474Epoch 00003: loss improved from 3.04942 to 3.04735, saving model to weights-03-3.0473.hdf5\n",
      "144324/144324 [==============================] - 117s 812us/step - loss: 3.0473\n",
      "Epoch 4/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0466Epoch 00004: loss improved from 3.04735 to 3.04659, saving model to weights-04-3.0466.hdf5\n",
      "144324/144324 [==============================] - 116s 803us/step - loss: 3.0466\n",
      "Epoch 5/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0459Epoch 00005: loss improved from 3.04659 to 3.04596, saving model to weights-05-3.0460.hdf5\n",
      "144324/144324 [==============================] - 116s 803us/step - loss: 3.0460\n",
      "Epoch 6/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0459Epoch 00006: loss improved from 3.04596 to 3.04595, saving model to weights-06-3.0460.hdf5\n",
      "144324/144324 [==============================] - 115s 798us/step - loss: 3.0460\n",
      "Epoch 7/20\n",
      "144240/144324 [============================>.] - ETA: 0s - loss: 3.0475- ETA - Epoch 00007: loss did not improve\n",
      "144324/144324 [==============================] - 114s 789us/step - loss: 3.0474\n",
      "Epoch 8/20\n",
      "118920/144324 [=======================>......] - ETA: 20s - loss: 3.0497"
     ]
    }
   ],
   "source": [
    "sentence_length = 100 \n",
    "file_name = \"wonder_clean.txt\"\n",
    "\n",
    "int_to_char = {}\n",
    "char_to_int = {}\n",
    "epochs = 20\n",
    "batch = 120\n",
    "\n",
    "def prepare_dataset(file_name, seq_length):\n",
    "    raw_text = open(file_name).read().lower()\n",
    "    unique_chars = list(set(raw_text))\n",
    "    #unique_chars = [c for c in unique_chars if str.isalnum(c)]\n",
    "    n_count = len(raw_text)\n",
    "    n_uniq_char = len(unique_chars)\n",
    "    print(\"Total characters in file\",n_count)\n",
    "    print(\"Total unique characters in file \",n_uniq_char)\n",
    "    char_to_int, int_to_char = get_lookups(unique_chars)\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(raw_text)-seq_length, 1):\n",
    "        phrase = raw_text[i:seq_length]\n",
    "        label_char = raw_text[i+seq_length]\n",
    "        sentences.append(phrase)\n",
    "        labels.append(label_char)\n",
    "    \n",
    "    n_sentences = len(sentences)\n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    #vectorization, create 3D matrix that LSTM can consume\n",
    "    #each data item has seq_length characters, each of which is encoded as 1-hot of length n_uniq_char\n",
    "    X = np.zeros( (n_sentences, seq_length, n_uniq_char) , dtype = np.bool)\n",
    "    y = np.zeros( (n_labels, n_uniq_char) , dtype = np.bool)\n",
    "    for s, sent in enumerate(sentences):\n",
    "        for t, c in enumerate(sent):\n",
    "            X[s, t, char_to_int[c]] = 1\n",
    "        y[s, char_to_int[labels[s]]] = 1\n",
    "    \n",
    "    return char_to_int, int_to_char, sentences, labels, X, y\n",
    "\n",
    "def get_lookups(uniq_chars):\n",
    "    char_to_int = {c:ind for ind, c in enumerate(uniq_chars) }\n",
    "    int_to_char = { ind:char for char,ind in char_to_int.items()}\n",
    "    print(\"\\n\")\n",
    "    print(\"Char lookup \",char_to_int,)\n",
    "    print(\"\\n\")\n",
    "    print(\"Int lookup \",int_to_char,)\n",
    "    return char_to_int, int_to_char\n",
    "\n",
    "\n",
    "def get_model(n_hidden, X, y, epochs, batch):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( LSTM(n_hidden, input_shape = (X.shape[1], X.shape[2])) )\n",
    "    model.add( Dropout(0.25) )\n",
    "    model.add( Dense(y.shape[1], activation = \"softmax\") )\n",
    "\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")\n",
    "    model.summary()\n",
    "    filepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch , callbacks=callbacks_list)\n",
    "    \n",
    "    return model\n",
    "\n",
    "char_to_int, int_to_char, sentencs, labels, X, y  = prepare_dataset(file_name,sentence_length)\n",
    "model = get_model(256, X, y, epochs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
